Minimum Requirements:
- OpenAPI compatible api calling for llm
- Async fastAPI server deployed to GCP
- Test performance

TODO:
- [ ] Set up CI/CD on GCP for cloud run
- [ ] Build llm wrapper with fastAPI
- [ ] testing framework
- [ ] deploy database for conversation history
- [ ] add CORS and auth
- [ ] add logging and monitoring
- [ ] Test performance
- [ ] Prompt tune
- [ ] Performance tune with caching
